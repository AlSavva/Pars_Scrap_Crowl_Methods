# Pars_Scrap_Crowl_Methods
### <center>Практические задания по курсу "Методы сбора и обработки данных при помощи Python" Geekbrains
Лекции разнесены по дирректориям, каждая содержит поддиректории "Materials" - с материалами лекций и "HomeWork" с выполненными заданиями.
### Lesson1. Основы клиент-серверного взаимодействия. Парсинг API  
#### Lesson1 task:
1.   Посмотреть документацию к API GitHub, разобраться как вывести список репозиториев для конкретного пользователя, сохранить JSON-вывод в файле *.json.
1.   Изучить список открытых API (https://www.programmableweb.com/category/all/apis). Найти среди них любое, требующее авторизацию (любого типа). Выполнить запросы к нему, пройдя авторизацию. Ответ сервера записать в файл.  

*Если нет желания заморачиваться с поиском, возьмите API вконтакте (https://vk.com/dev/first_guide). Сделайте запрос, чтобы получить список всех сообществ на которые вы подписаны.*

### Lesson2. Парсинг HTML. BeautifulSoup, MongoDB  
#### Lesson2 task:
* <ins>Вариант 1</ins>  
Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы) с сайтов Superjob и HH. Приложение должно анализировать несколько страниц сайта (также вводим через input или аргументы). Получившийся список должен содержать в себе минимум:  
  * Наименование вакансии.
  * Предлагаемую зарплату (отдельно минимальную и максимальную).
  * Ссылку на саму вакансию.
  * Сайт, откуда собрана вакансия.     

*По желанию можно добавить ещё параметры вакансии (например, работодателя и расположение). Структура должна быть одинаковая для вакансий с обоих сайтов. Общий результат можно вывести с помощью dataFrame через pandas.*  

* <ins>Вариант 2</ins>  
Необходимо собрать информацию по продуктам питания с сайтов:  
[Роскачество официальный сайт. Исследование качества продуктов питания | Рейтинг товаров.](https://rskrf.ru/ratings/produkty-pitaniya/)    
[Список протестированных продуктов на сайте Росконтроль.рф](https://roscontrol.com/category/produkti/#)  
Получившийся список должен содержать:
   * Наименование продукта.
   * Категорию продукта (например «Бакалея»).
   * Подкатегорию продукта (например «Рис круглозерный»).
   * Параметр «Безопасность».
   * Параметр «Качество».
  * Общий балл.
  * Сайт, откуда получена информация.     

*Структура должна быть одинаковая для продуктов с обоих сайтов. Общий результат можно вывести с помощью dataFrame через Pandas.*  

### Lesson3. Системы управления базами данных MongoDB и SQLite в Python  
#### Lesson3 task:

1.    Развернуть у себя на компьютере/виртуальной машине/хостинге MongoDB и реализовать функцию, записывающую собранные вакансии в созданную БД.
2.    Написать функцию, которая производит поиск и выводит на экран вакансии с заработной платой больше введённой суммы.
3.    Написать функцию, которая будет добавлять в вашу базу данных только новые вакансии с сайта.  

### Lesson4. Парсинг HTML. XPath  
#### Lesson4 task:  

1.    Написать приложение, которое собирает основные новости с сайтов mail.ru, lenta.ru, yandex-новости. Для парсинга использовать XPath. Структура данных должна содержать:  
 * название источника;  
 * наименование новости;  
 * ссылку на новость;  
 * дата публикации.      
2.    Сложить собранные данные в БД MongoDB.  

### Lesson5. Selenium в Python  
#### Lesson5 task:  

1. Написать программу, которая собирает входящие письма из своего или тестового почтового ящика и сложить данные о письмах в базу данных:  
 * отправитель,
 * дата отправки,
 * тема письма,
 * текст письма.  
2. Написать программу, которая собирает «Хиты продаж» с сайта техники mvideo и складывает данные в БД. Магазины можно выбрать свои. Главный критерий выбора: динамически загружаемые товары  

### Lesson6. Scrapy 
#### Lesson5 task:  

**<ins>I вариант</ins>**
 1. Доработать паука в имеющемся проекте, чтобы он формировал item по структуре:
  * Наименование вакансии
  * Зарплата от
  * Зарплата до
  * Ссылку на саму вакансию
  * Сайт откуда собрана вакансия  
 2. Собранная информация дожна складываться в базу данных(любую)  
 3. Создать в имеющемся проекте второго паука по сбору вакансий с сайта superjob. Паук должен формировать item'ы по аналогичной структуре и складывать данные также в БД

**<ins>II вариант</ins>**
 1. Создать двух пауков по сбору данных о книгах с сайтов labirint.ru и book24.ru
 2. Каждый паук должен собирать:
  * Ссылку на книгу
  * Наименование книги
  * Автор(ы)
  * Основную цену
  * Цену со скидкой
  * Рейтинг книги
 3. Собранная информация дожна складываться в базу данных
 
 ### Lesson7. Scrapy. Парсинг фото и файлов. 
 #### Lesson7 task:  
 
1. Написать паука, собирающего информацию о товарах по выбранной категории с сайта "Леруа Мерлен". Должна быть собрана информация о товаре, его характеристики и изображения товаров. Собранные данные разместить в БД.  

 ### Lesson8. Работа с данными.    
 #### Lesson8 task:  

1. Написать приложение, которое будет проходиться по указанному списку двух и/или более пользователей Instagramm, и собирать данные об их подписчиках и подписках.
2.  По каждому пользователю, который является подписчиком или на которого подписан исследуемый объект нужно извлечь имя, id, фото (остальные данные по желанию). Фото можно дополнительно скачать.
3.  Собранные данные необходимо сложить в базу данных. Структуру данных нужно заранее продумать, чтобы:
4.  Написать запрос к базе, который вернет список подписчиков только указанного пользователя
5.  Написать запрос к базе, который вернет список профилей, на кого подписан указанный пользователь
 
